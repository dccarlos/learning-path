{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"bigdata/apache-avro/introduction/","text":"Introduction to Apache Avro What's Apache Avro? Apache Avro is a language-neutral data serialization system. The project was created by Doug Cutting (the creator of Hadoop) to address the major downside of Hadoop Writables: lack of language portability. Why a new data serialization system? Avro has a set of features that, taken together, differentiate it from other systems such as Apache Thrift or Google\u2019s Protocol Buffers. Like in these systems and others, Avro data is described using a language-independent schema . However, unlike in some other systems, code generation is optional in Avro, which means you can read and write data that conforms to a given schema even if your code has not seen that particular schema before . To achieve this, Avro assumes that the schema is always present\u2014at both read and write time\u2014which makes for a very compact encoding, since encoded values do not need to be tagged with a field identifier. With code-generation usually means that before compiling your Java application, you have an Avro schema available. You, as a developer, will use an Avro compiler to generate a class for each record in the schema and you use these classes in your application. In the referenced link, the author does this: java -jar avro-tools-1.7.5.jar compile schema student.avsc , and then uses the student_marks class directly. In this case, each instance of the class student_marks inherits from SpecificRecord , with custom methods for accessing the data inside (such as getStudentId() to fetch the student_id field). Without code-generation usually means that your application doesn't have any specific necessary schema (for example, it can treat different kinds of data). In this case, there's no student class generated, but you can still read Avro records in an Avro container. You won't have instances of student , but instances of GenericRecord . There won't be any helpful methods like getStudentId() , but you can use methods get(\"student_marks\") or get(0) . Schemas Avro schemas are usually written in JSON, and data is usually encoded using a binary format, but there are other options, too. There is a higher-level language called Avro IDL for writing schemas in a C-like language that is more familiar to developers. There is also a JSON-based data encoder, which, being human readable, is useful for prototyping and debugging Avro data. The Avro specification precisely defines the binary format that all implementations must support. It also specifies many of the other features of Avro that implementations should support. One area that the specification does not rule on, however, is APIs: implementations have complete latitude in the APIs they expose for working with Avro data, since each one is necessarily language specific. Schema resolution Within certain carefully defined constraints, the schema used to read data need not be identical to the schema that was used to write the data. This is the mechanism by which Avro supports schema evolution. For example, a new, optional field may be added to a record by declaring it in the schema used to read the old data. New and old clients alike will be able to read the old data, while new clients can write new data that uses the new field. Conversely, if an old client sees newly encoded data, it will gracefully ignore the new field and carry on processing as it would have done with old data. Avro specifies an object container format for sequences of objects, similar to Hadoop\u2019s sequence file. An Avro datafile has a metadata section where the schema is stored, which makes the file self-describing. Avro datafiles support compression and are splittable, which is crucial for a MapReduce data input format. In fact, support goes beyond MapReduce: all of the data processing frameworks in this book (Pig, Hive, Crunch, Spark) can read and write Avro datafiles. Avro Data Types and Schemas Each Avro language API has a representation for each Avro type that is specific to the language. For example, Avro\u2019s double type is represented in C, C++, and Java by a double , in Python by a float , and in Ruby by a Float . Generic Mapping All languages support a dynamic mapping, which can be used even when the schema is not known ahead of runtime. Specific Mapping The Java and C++ implementations can generate code to represent the data for an Avro schema. It is an optimization that is useful when you have a copy of the schema before you read or write data. Generated classes also provide a more domain-oriented API for user code than Generic ones. Reflect Mapping (Java specific one) Maps Avro types onto preexisting Java types using reflection. It is slower than the Generic and Specific mappings but can be a convenient way of defining a type, since Avro can infer a schema automatically. Avro string can be represented by either Java String or the Avro Utf8 Java type. The reason to use Utf8 is efficiency: because it is mutable, a single Utf8 instance may be reused for reading or writing a series of values. Also, Java String decodes UTF-8 at object construction time, whereas Avro Utf8 does it lazily, which can increase performance in some cases. Utf8 implements Java\u2019s java.lang.CharSequence interface, which allows some interoperability with Java libraries. In other cases, it may be necessary to convert Utf8 instances to String objects by calling its toString() method. Utf8 is the default for Generic and Specific, but it\u2019s possible to use String for a particular mapping. There are a couple of ways to achieve this. The first is to set the avro.java.string property in the schema to String: json { \"type\": \"string\", \"avro.java.string\": \"String\" } In-Memory Serialization and Deserialization @Test public void testEncoderDecoder() throws IOException { System.out.println(\"Encode/Decode test\"); Schema.Parser parser = new Schema.Parser(); Schema schema = parser.parse(getClass().getResourceAsStream(\"/avro/user.avsc\")); try (ByteArrayOutputStream outputStream = new ByteArrayOutputStream()) { DatumWriter<GenericRecord> writer = new GenericDatumWriter<>(schema); GenericRecord user1 = new GenericData.Record(schema); user1.put(\"name\", \"Armando\"); user1.put(\"favoriteNumber\", 11); user1.put(\"favoriteColor\", \"PINK\"); GenericRecord user2 = new GenericData.Record(schema); user2.put(\"name\", \"Rosana\"); user2.put(\"favoriteNumber\", 14); user2.put(\"favoriteColor\", \"GREY\"); Encoder encoder = EncoderFactory.get().jsonEncoder(schema, outputStream); writer.write(user1, encoder); writer.write(user2, encoder); encoder.flush(); final String jsonRecords = outputStream.toString(); System.out.println(\"Encoded avro => json:\\n\" + jsonRecords); DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema); Decoder decoder = DecoderFactory.get().jsonDecoder(schema, jsonRecords); GenericRecord parsedUser1 = datumReader.read(null, decoder); GenericRecord parsedUser2 = datumReader.read(null, decoder); // GenericRecord parsedNull = datumReader.read(null, decoder); // <= Exception! System.out.println(); System.out.println(\"Decoded json => avro:\"); System.out.println(parsedUser1); System.out.println(parsedUser2); } catch (IOException ioe) { ioe.printStackTrace(); } } Avro Datafiles Avro\u2019s object container file format is for storing sequences of Avro objects. A datafile has a header containing metadata, including the Avro schema and a sync marker , followed by a series of (optionally compressed) blocks containing the serialized Avro objects. Blocks are separated by a sync marker that is unique to the file (the marker for a particular file is found in the header) and that permits rapid resynchronization with a block boundary after seeking to an arbitrary point in the file, such as an HDFS block boundary. Thus, Avro datafiles are splittable. Schema Resolution We can choose to use a different schema for reading the data back (the reader\u2019s schema ) from the one we used to write it (the writer\u2019s schema ). This is a powerful tool because it enables schema evolution. To illustrate, consider a new schema for string pairs with an added description field: { \"type\": \"record\", \"name\": \"StringPair\", \"doc\": \"A pair of strings with an added field.\", \"fields\": [ {\"name\": \"left\", \"type\": \"string\"}, {\"name\": \"right\", \"type\": \"string\"}, {\"name\": \"description\", \"type\": \"string\", \"default\": \"\"} ] } We can use this schema to read the data we serialized earlier because, crucially, we have given the description field a default value (the empty string), which Avro will use when there is no such field defined in the records it is reading. Had we omitted the default attribute, we would get an error when trying to read the old data. To make the default value null rather than the empty string, we would instead define the description field using a union with the null Avro type: json {\"name\": \"description\", \"type\": [\"null\", \"string\"], \"default\": null} When the reader\u2019s schema is different from the writer\u2019s, we use the constructor for GenericDatumReader that takes two schema objects, the writer\u2019s and the reader\u2019s, in that order: DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(schema, newSchema); Decoder decoder = DecoderFactory.get().binaryDecoder(out.toByteArray(), null); GenericRecord result = reader.read(null, decoder); assertThat(result.get(\"left\").toString(), is(\"L\")); assertThat(result.get(\"right\").toString(), is(\"R\")); assertThat(result.get(\"description\").toString(), is(\"\")); For datafiles, which have the writer\u2019s schema stored in the metadata, we only need to specify the reader\u2019s schema explicitly, which we can do by passing null for the writer\u2019s schema: DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(null, newSchema); Projection or sub-schema Another common use of a different reader\u2019s schema is to drop fields in a record, an operation called projection . This is useful when you have records with a large number of fields and you want to read only some of them. For example, this schema can be used to get only the right field of a StringPair : { \"type\": \"record\", \"name\": \"StringPair\", \"doc\": \"A pair of strings with an added field.\", \"fields\": [ {\"name\": \"right\", \"type\": \"string\"} ] } Rules of resolution The rules for schema resolution have a direct bearing on how schemas may evolve from one version to the next, and are spelled out in the Avro specification for all Avro types. Another useful technique for evolving Avro schemas is the use of name aliases . Aliases allow you to use different names in the schema used to read the Avro data than in the schema originally used to write the data. For example, the following reader\u2019s schema can be used to read StringPair data with the new field names first and second instead of left and right (which are what it was written with): { \"type\": \"record\", \"name\": \"StringPair\", \"doc\": \"A pair of strings with an added field.\", \"fields\": [ {\"name\": \"first\", \"type\": \"string\", \"aliases\": [\"left\"]}, {\"name\": \"second\", \"type\": \"string\", \"aliases\": [\"right\"]} ] } Note that the aliases are used to translate (at read time) the writer\u2019s schema into the reader\u2019s, but the alias names are not available to the reader. In this example, the reader cannot use the field names left and right , because they have already been translated to first and second . Sort Order","title":"Introduction to Avro"},{"location":"bigdata/apache-avro/introduction/#introduction-to-apache-avro","text":"","title":"Introduction to Apache Avro"},{"location":"bigdata/apache-avro/introduction/#whats-apache-avro","text":"Apache Avro is a language-neutral data serialization system. The project was created by Doug Cutting (the creator of Hadoop) to address the major downside of Hadoop Writables: lack of language portability.","title":"What's Apache Avro?"},{"location":"bigdata/apache-avro/introduction/#why-a-new-data-serialization-system","text":"Avro has a set of features that, taken together, differentiate it from other systems such as Apache Thrift or Google\u2019s Protocol Buffers. Like in these systems and others, Avro data is described using a language-independent schema . However, unlike in some other systems, code generation is optional in Avro, which means you can read and write data that conforms to a given schema even if your code has not seen that particular schema before . To achieve this, Avro assumes that the schema is always present\u2014at both read and write time\u2014which makes for a very compact encoding, since encoded values do not need to be tagged with a field identifier. With code-generation usually means that before compiling your Java application, you have an Avro schema available. You, as a developer, will use an Avro compiler to generate a class for each record in the schema and you use these classes in your application. In the referenced link, the author does this: java -jar avro-tools-1.7.5.jar compile schema student.avsc , and then uses the student_marks class directly. In this case, each instance of the class student_marks inherits from SpecificRecord , with custom methods for accessing the data inside (such as getStudentId() to fetch the student_id field). Without code-generation usually means that your application doesn't have any specific necessary schema (for example, it can treat different kinds of data). In this case, there's no student class generated, but you can still read Avro records in an Avro container. You won't have instances of student , but instances of GenericRecord . There won't be any helpful methods like getStudentId() , but you can use methods get(\"student_marks\") or get(0) .","title":"Why a new data serialization system?"},{"location":"bigdata/apache-avro/introduction/#schemas","text":"Avro schemas are usually written in JSON, and data is usually encoded using a binary format, but there are other options, too. There is a higher-level language called Avro IDL for writing schemas in a C-like language that is more familiar to developers. There is also a JSON-based data encoder, which, being human readable, is useful for prototyping and debugging Avro data. The Avro specification precisely defines the binary format that all implementations must support. It also specifies many of the other features of Avro that implementations should support. One area that the specification does not rule on, however, is APIs: implementations have complete latitude in the APIs they expose for working with Avro data, since each one is necessarily language specific.","title":"Schemas"},{"location":"bigdata/apache-avro/introduction/#schema-resolution","text":"Within certain carefully defined constraints, the schema used to read data need not be identical to the schema that was used to write the data. This is the mechanism by which Avro supports schema evolution. For example, a new, optional field may be added to a record by declaring it in the schema used to read the old data. New and old clients alike will be able to read the old data, while new clients can write new data that uses the new field. Conversely, if an old client sees newly encoded data, it will gracefully ignore the new field and carry on processing as it would have done with old data. Avro specifies an object container format for sequences of objects, similar to Hadoop\u2019s sequence file. An Avro datafile has a metadata section where the schema is stored, which makes the file self-describing. Avro datafiles support compression and are splittable, which is crucial for a MapReduce data input format. In fact, support goes beyond MapReduce: all of the data processing frameworks in this book (Pig, Hive, Crunch, Spark) can read and write Avro datafiles.","title":"Schema resolution"},{"location":"bigdata/apache-avro/introduction/#avro-data-types-and-schemas","text":"Each Avro language API has a representation for each Avro type that is specific to the language. For example, Avro\u2019s double type is represented in C, C++, and Java by a double , in Python by a float , and in Ruby by a Float .","title":"Avro Data Types and Schemas"},{"location":"bigdata/apache-avro/introduction/#generic-mapping","text":"All languages support a dynamic mapping, which can be used even when the schema is not known ahead of runtime.","title":"Generic Mapping"},{"location":"bigdata/apache-avro/introduction/#specific-mapping","text":"The Java and C++ implementations can generate code to represent the data for an Avro schema. It is an optimization that is useful when you have a copy of the schema before you read or write data. Generated classes also provide a more domain-oriented API for user code than Generic ones.","title":"Specific Mapping"},{"location":"bigdata/apache-avro/introduction/#reflect-mapping-java-specific-one","text":"Maps Avro types onto preexisting Java types using reflection. It is slower than the Generic and Specific mappings but can be a convenient way of defining a type, since Avro can infer a schema automatically. Avro string can be represented by either Java String or the Avro Utf8 Java type. The reason to use Utf8 is efficiency: because it is mutable, a single Utf8 instance may be reused for reading or writing a series of values. Also, Java String decodes UTF-8 at object construction time, whereas Avro Utf8 does it lazily, which can increase performance in some cases. Utf8 implements Java\u2019s java.lang.CharSequence interface, which allows some interoperability with Java libraries. In other cases, it may be necessary to convert Utf8 instances to String objects by calling its toString() method. Utf8 is the default for Generic and Specific, but it\u2019s possible to use String for a particular mapping. There are a couple of ways to achieve this. The first is to set the avro.java.string property in the schema to String: json { \"type\": \"string\", \"avro.java.string\": \"String\" }","title":"Reflect Mapping (Java specific one)"},{"location":"bigdata/apache-avro/introduction/#in-memory-serialization-and-deserialization","text":"@Test public void testEncoderDecoder() throws IOException { System.out.println(\"Encode/Decode test\"); Schema.Parser parser = new Schema.Parser(); Schema schema = parser.parse(getClass().getResourceAsStream(\"/avro/user.avsc\")); try (ByteArrayOutputStream outputStream = new ByteArrayOutputStream()) { DatumWriter<GenericRecord> writer = new GenericDatumWriter<>(schema); GenericRecord user1 = new GenericData.Record(schema); user1.put(\"name\", \"Armando\"); user1.put(\"favoriteNumber\", 11); user1.put(\"favoriteColor\", \"PINK\"); GenericRecord user2 = new GenericData.Record(schema); user2.put(\"name\", \"Rosana\"); user2.put(\"favoriteNumber\", 14); user2.put(\"favoriteColor\", \"GREY\"); Encoder encoder = EncoderFactory.get().jsonEncoder(schema, outputStream); writer.write(user1, encoder); writer.write(user2, encoder); encoder.flush(); final String jsonRecords = outputStream.toString(); System.out.println(\"Encoded avro => json:\\n\" + jsonRecords); DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema); Decoder decoder = DecoderFactory.get().jsonDecoder(schema, jsonRecords); GenericRecord parsedUser1 = datumReader.read(null, decoder); GenericRecord parsedUser2 = datumReader.read(null, decoder); // GenericRecord parsedNull = datumReader.read(null, decoder); // <= Exception! System.out.println(); System.out.println(\"Decoded json => avro:\"); System.out.println(parsedUser1); System.out.println(parsedUser2); } catch (IOException ioe) { ioe.printStackTrace(); } }","title":"In-Memory Serialization and Deserialization"},{"location":"bigdata/apache-avro/introduction/#avro-datafiles","text":"Avro\u2019s object container file format is for storing sequences of Avro objects. A datafile has a header containing metadata, including the Avro schema and a sync marker , followed by a series of (optionally compressed) blocks containing the serialized Avro objects. Blocks are separated by a sync marker that is unique to the file (the marker for a particular file is found in the header) and that permits rapid resynchronization with a block boundary after seeking to an arbitrary point in the file, such as an HDFS block boundary. Thus, Avro datafiles are splittable.","title":"Avro Datafiles"},{"location":"bigdata/apache-avro/introduction/#schema-resolution_1","text":"We can choose to use a different schema for reading the data back (the reader\u2019s schema ) from the one we used to write it (the writer\u2019s schema ). This is a powerful tool because it enables schema evolution. To illustrate, consider a new schema for string pairs with an added description field: { \"type\": \"record\", \"name\": \"StringPair\", \"doc\": \"A pair of strings with an added field.\", \"fields\": [ {\"name\": \"left\", \"type\": \"string\"}, {\"name\": \"right\", \"type\": \"string\"}, {\"name\": \"description\", \"type\": \"string\", \"default\": \"\"} ] } We can use this schema to read the data we serialized earlier because, crucially, we have given the description field a default value (the empty string), which Avro will use when there is no such field defined in the records it is reading. Had we omitted the default attribute, we would get an error when trying to read the old data. To make the default value null rather than the empty string, we would instead define the description field using a union with the null Avro type: json {\"name\": \"description\", \"type\": [\"null\", \"string\"], \"default\": null} When the reader\u2019s schema is different from the writer\u2019s, we use the constructor for GenericDatumReader that takes two schema objects, the writer\u2019s and the reader\u2019s, in that order: DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(schema, newSchema); Decoder decoder = DecoderFactory.get().binaryDecoder(out.toByteArray(), null); GenericRecord result = reader.read(null, decoder); assertThat(result.get(\"left\").toString(), is(\"L\")); assertThat(result.get(\"right\").toString(), is(\"R\")); assertThat(result.get(\"description\").toString(), is(\"\")); For datafiles, which have the writer\u2019s schema stored in the metadata, we only need to specify the reader\u2019s schema explicitly, which we can do by passing null for the writer\u2019s schema: DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(null, newSchema);","title":"Schema Resolution"},{"location":"bigdata/apache-avro/introduction/#projection-or-sub-schema","text":"Another common use of a different reader\u2019s schema is to drop fields in a record, an operation called projection . This is useful when you have records with a large number of fields and you want to read only some of them. For example, this schema can be used to get only the right field of a StringPair : { \"type\": \"record\", \"name\": \"StringPair\", \"doc\": \"A pair of strings with an added field.\", \"fields\": [ {\"name\": \"right\", \"type\": \"string\"} ] }","title":"Projection or sub-schema"},{"location":"bigdata/apache-avro/introduction/#rules-of-resolution","text":"The rules for schema resolution have a direct bearing on how schemas may evolve from one version to the next, and are spelled out in the Avro specification for all Avro types. Another useful technique for evolving Avro schemas is the use of name aliases . Aliases allow you to use different names in the schema used to read the Avro data than in the schema originally used to write the data. For example, the following reader\u2019s schema can be used to read StringPair data with the new field names first and second instead of left and right (which are what it was written with): { \"type\": \"record\", \"name\": \"StringPair\", \"doc\": \"A pair of strings with an added field.\", \"fields\": [ {\"name\": \"first\", \"type\": \"string\", \"aliases\": [\"left\"]}, {\"name\": \"second\", \"type\": \"string\", \"aliases\": [\"right\"]} ] } Note that the aliases are used to translate (at read time) the writer\u2019s schema into the reader\u2019s, but the alias names are not available to the reader. In this example, the reader cannot use the field names left and right , because they have already been translated to first and second .","title":"Rules of resolution"},{"location":"bigdata/apache-avro/introduction/#sort-order","text":"","title":"Sort Order"},{"location":"soft-eng/devops/introduction/","text":"Introduction to DevOps What's DevOps? It is the outcome of applying the most trusted principles from the domain of physical manufacturing and leadership to the IT value stream. In other words, applying lean principles to technology value stream and the three ways: Flow, feedback, continual-learning and experimentation. The result is world-class quality, reliability, stability, and security at ever lower cost and effort; and accelerated flow and reliability throughout the technology value stream, including Product Management, Development, QA, IT Operations, and Infosec. Lean principles Focus on how to create value for the customer through systems thinking by creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual. Flow Accelerate the delivery of work from Development \u2192 Operations \u2192 Customers Feedback Enable us to create ever safer systems of work Continual Learning and Experimentation Promotes a high-trust culture and a scientific approach to organizational improvement risk-taking as part of our daily work Lean principles Value stream Is the sequence of activities an organization undertakes to deliver upon a customer request. In other words, the sequence of activities required to design, produce, and deliver a good or service to a customer, including the dual flows of information and material. In manufacturing operations, the value stream is often easy to see and observe: it starts when a customer order is received and the raw materials are released onto the plant floor. Technology value stream In DevOps, we typically define our technology value stream as the process required to convert a business hypothesis into a technology-enabled service that delivers value to the customer. The input to our process is the formulation of a business objective, concept, idea, or hypothesis, and starts when we accept the work in Development, adding it to our committed backlog of work. From there, Development teams that follow a typical Agile or iterative process will likely transform that idea into user stories and some sort of feature specification, which is then implemented in code into the application or service being built. The code is then checked in to the version control repository, where each change is integrated and tested with the rest of the software system . Because value is created only when our services are running in production , we must ensure that we are not only delivering fast flow, but that our deployments can also be performed without causing chaos and disruptions such as service outages, service impairments, or security or compliance failures. First phase of technology value stream includes Design and Development is akin to Lean Product Development and is highly variable and highly uncertain, often requiring high degrees of creativity and work that may never be performed again, resulting in high variability of process times. In contrast, the second phase of work, which includes Testing and Operations , is akin to Lean Manufacturing . It requires creativity and expertise, and strives to be predictable and mechanistic, with the goal of achieving work outputs with minimized variability. Deployment lead time It is a subset of the value stream described above. This value stream begins when any engineer in our value stream (which includes Development, QA, IT Operations, and Infosec) checks a change into version control and ends when that change is successfully running in production , providing value to the customer and generating useful feedback and telemetry. Instead of large batches of work being processed sequentially through the design/development value stream and then through the test/operations value stream (such as when we have a large batch waterfall process or long-lived feature branches), our goal is to have testing and operations happening simultaneously with design/development, enabling fast flow and high quality. This method succeeds when we work in small batches and build quality into every part of our value stream . Lead Time vs. Processing Time Because lead time is what the customer experiences, we typically focus our process improvement attention there instead of on process time. Deployment lead times requiring months This is especially common in large, complex organizations that are working with tightly-coupled, monolithic applications, often with scarce integration test environments, long test and production environment lead times, high reliance on manual testing, and multiple required approval processes. Deployment lead times of minutes In this scenario developers receive fast, constant feedback on their work, which enables them to quickly and independently implement, integrate, and validate their code, and have the code deployed into the production environment (either by deploying the code themselves or by others). We achieve this by continually checking small code changes into our version control repository, performing automated and exploratory testing against it, and deploying it into production . This enables us to have a high degree of confidence that our changes will operate as designed in production and that any problems can be quickly detected and corrected. %C/A (percent complete and accurate) This is the third key metric in the technology value stream, it reflects the quality of the output of each step in our value stream and can be obtained by asking downstream customers what percentage of the time they receive work that is \u2018usable as is,\u2019 meaning that they can do their work without having to correct the information that was provided, add missing information that should have been supplied, or clarify information that should have and could have been clearer. Three ways: The principles underpinning devops ( The Phoenix Project ) The First Way enables fast left-to-right flow of work from Development to Operations to the customer. The necessary practices include continuous build, integration, and deployment, creating environments on demand, limiting work in process, and building safe systems and organizations that are safe to change. The Second Way enables the fast and constant flow of feedback from right to left at all stages of our value stream. It requires that we amplify feedback to prevent problems from happening again, or enable faster detection and recovery. By doing this, we create quality at the source and generate or embed knowledge where it is needed\u2014this allows us to create ever-safer systems of work where problems are found and fixed long before a catastrophic failure occurs. The necessary practices include \u201cstopping the production line\u201d when our builds and tests fail in the deployment pipeline; constantly elevating the improvement of daily work over daily work; creating fast automated test suites to ensure that code is always in a potentially deployable state; creating shared goals and shared pain between Development and IT Operations; and creating pervasive production telemetry so that everyone can see whether code and environments are operating as designed and that customer goals are being met. The Third Way enables the creation of a generative, high-trust culture that supports a dynamic, disciplined, and scientific approach to experimentation and risk-taking, facilitating the creation of organizational learning, both from our successes and failures. Furthermore, by continually shortening and amplifying our feedback loops, we create ever-safer systems of work and are better able to take risks and perform experiments that help us learn faster than our competition and win in the marketplace. The necessary practices include creating a culture of innovation and risk taking (as opposed to fear or mindless order taking) and high trust (as opposed to low trust, command-and-control), allocating at least twenty percent of Development and IT Operations cycles towards nonfunctional requirements, and constant reinforcement that improvements are encouraged and celebrated.","title":"Introduction to DevOps"},{"location":"soft-eng/devops/introduction/#introduction-to-devops","text":"","title":"Introduction to DevOps"},{"location":"soft-eng/devops/introduction/#whats-devops","text":"It is the outcome of applying the most trusted principles from the domain of physical manufacturing and leadership to the IT value stream. In other words, applying lean principles to technology value stream and the three ways: Flow, feedback, continual-learning and experimentation. The result is world-class quality, reliability, stability, and security at ever lower cost and effort; and accelerated flow and reliability throughout the technology value stream, including Product Management, Development, QA, IT Operations, and Infosec. Lean principles Focus on how to create value for the customer through systems thinking by creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual. Flow Accelerate the delivery of work from Development \u2192 Operations \u2192 Customers Feedback Enable us to create ever safer systems of work Continual Learning and Experimentation Promotes a high-trust culture and a scientific approach to organizational improvement risk-taking as part of our daily work","title":"What's DevOps?"},{"location":"soft-eng/devops/introduction/#lean-principles","text":"","title":"Lean principles"},{"location":"soft-eng/devops/introduction/#value-stream","text":"Is the sequence of activities an organization undertakes to deliver upon a customer request. In other words, the sequence of activities required to design, produce, and deliver a good or service to a customer, including the dual flows of information and material. In manufacturing operations, the value stream is often easy to see and observe: it starts when a customer order is received and the raw materials are released onto the plant floor.","title":"Value stream"},{"location":"soft-eng/devops/introduction/#technology-value-stream","text":"In DevOps, we typically define our technology value stream as the process required to convert a business hypothesis into a technology-enabled service that delivers value to the customer. The input to our process is the formulation of a business objective, concept, idea, or hypothesis, and starts when we accept the work in Development, adding it to our committed backlog of work. From there, Development teams that follow a typical Agile or iterative process will likely transform that idea into user stories and some sort of feature specification, which is then implemented in code into the application or service being built. The code is then checked in to the version control repository, where each change is integrated and tested with the rest of the software system . Because value is created only when our services are running in production , we must ensure that we are not only delivering fast flow, but that our deployments can also be performed without causing chaos and disruptions such as service outages, service impairments, or security or compliance failures. First phase of technology value stream includes Design and Development is akin to Lean Product Development and is highly variable and highly uncertain, often requiring high degrees of creativity and work that may never be performed again, resulting in high variability of process times. In contrast, the second phase of work, which includes Testing and Operations , is akin to Lean Manufacturing . It requires creativity and expertise, and strives to be predictable and mechanistic, with the goal of achieving work outputs with minimized variability.","title":"Technology value stream"},{"location":"soft-eng/devops/introduction/#deployment-lead-time","text":"It is a subset of the value stream described above. This value stream begins when any engineer in our value stream (which includes Development, QA, IT Operations, and Infosec) checks a change into version control and ends when that change is successfully running in production , providing value to the customer and generating useful feedback and telemetry. Instead of large batches of work being processed sequentially through the design/development value stream and then through the test/operations value stream (such as when we have a large batch waterfall process or long-lived feature branches), our goal is to have testing and operations happening simultaneously with design/development, enabling fast flow and high quality. This method succeeds when we work in small batches and build quality into every part of our value stream .","title":"Deployment lead time"},{"location":"soft-eng/devops/introduction/#lead-time-vs-processing-time","text":"Because lead time is what the customer experiences, we typically focus our process improvement attention there instead of on process time.","title":"Lead Time vs. Processing Time"},{"location":"soft-eng/devops/introduction/#deployment-lead-times-requiring-months","text":"This is especially common in large, complex organizations that are working with tightly-coupled, monolithic applications, often with scarce integration test environments, long test and production environment lead times, high reliance on manual testing, and multiple required approval processes.","title":"Deployment lead times requiring months"},{"location":"soft-eng/devops/introduction/#deployment-lead-times-of-minutes","text":"In this scenario developers receive fast, constant feedback on their work, which enables them to quickly and independently implement, integrate, and validate their code, and have the code deployed into the production environment (either by deploying the code themselves or by others). We achieve this by continually checking small code changes into our version control repository, performing automated and exploratory testing against it, and deploying it into production . This enables us to have a high degree of confidence that our changes will operate as designed in production and that any problems can be quickly detected and corrected.","title":"Deployment lead times of minutes"},{"location":"soft-eng/devops/introduction/#ca-percent-complete-and-accurate","text":"This is the third key metric in the technology value stream, it reflects the quality of the output of each step in our value stream and can be obtained by asking downstream customers what percentage of the time they receive work that is \u2018usable as is,\u2019 meaning that they can do their work without having to correct the information that was provided, add missing information that should have been supplied, or clarify information that should have and could have been clearer.","title":"%C/A (percent complete and accurate)"},{"location":"soft-eng/devops/introduction/#three-ways-the-principles-underpinning-devops-the-phoenix-project","text":"The First Way enables fast left-to-right flow of work from Development to Operations to the customer. The necessary practices include continuous build, integration, and deployment, creating environments on demand, limiting work in process, and building safe systems and organizations that are safe to change. The Second Way enables the fast and constant flow of feedback from right to left at all stages of our value stream. It requires that we amplify feedback to prevent problems from happening again, or enable faster detection and recovery. By doing this, we create quality at the source and generate or embed knowledge where it is needed\u2014this allows us to create ever-safer systems of work where problems are found and fixed long before a catastrophic failure occurs. The necessary practices include \u201cstopping the production line\u201d when our builds and tests fail in the deployment pipeline; constantly elevating the improvement of daily work over daily work; creating fast automated test suites to ensure that code is always in a potentially deployable state; creating shared goals and shared pain between Development and IT Operations; and creating pervasive production telemetry so that everyone can see whether code and environments are operating as designed and that customer goals are being met. The Third Way enables the creation of a generative, high-trust culture that supports a dynamic, disciplined, and scientific approach to experimentation and risk-taking, facilitating the creation of organizational learning, both from our successes and failures. Furthermore, by continually shortening and amplifying our feedback loops, we create ever-safer systems of work and are better able to take risks and perform experiments that help us learn faster than our competition and win in the marketplace. The necessary practices include creating a culture of innovation and risk taking (as opposed to fear or mindless order taking) and high trust (as opposed to low trust, command-and-control), allocating at least twenty percent of Development and IT Operations cycles towards nonfunctional requirements, and constant reinforcement that improvements are encouraged and celebrated.","title":"Three ways: The principles underpinning devops (The Phoenix Project)"}]}